{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Solution_PyTorch_Dataset_Dataloader",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-mOZ6oR8q6K",
        "colab_type": "text"
      },
      "source": [
        "## Use PyTorch `Dataset` and `Dataloader` with a structured dataset\n",
        "\n",
        "* **IMPORTANT:** replace the `URL = None` in the following cell with the  value for the URL from your learning materials."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oKWxWlmun66",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import torch as pt\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "pt.set_default_dtype(pt.float64)\n",
        "\n",
        "URL = None\n",
        "\n",
        "assert URL and (type(URL) is str), \"Be sure to initialize URL using the value from your learning materials\"\n",
        "os.environ['URL'] = URL"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKgUBHm-9AEJ",
        "colab_type": "text"
      },
      "source": [
        "Download and unzip the contents of the `URL` to the `data` subdirectory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVWk7ZZ6tcox",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "wget -q $URL\n",
        "mkdir -p data\n",
        "find *.zip | xargs unzip -o -d data/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4p8W9ZSL9I9w",
        "colab_type": "text"
      },
      "source": [
        "Read the files that match `part-*.csv` from the `data` subdirectory into a Pandas data frame named `df`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3fr0_i_YEFf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "df = pd.concat(\n",
        "    pd.read_csv(file) for file in Path('data/').glob('part-*.csv')\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RULBK-A9X7D",
        "colab_type": "text"
      },
      "source": [
        "## Explore the `df` data frame, including the column names, the first few rows of the dataset, and the data frame's memory usage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vDc_ZNI9ilK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "3540d25b-fb66-4f2d-d125-33be9ed894da"
      },
      "source": [
        "df[:5]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fareamount</th>\n",
              "      <th>origindatetime_tr</th>\n",
              "      <th>origin_block_latitude</th>\n",
              "      <th>origin_block_longitude</th>\n",
              "      <th>destination_block_latitude</th>\n",
              "      <th>destination_block_longitude</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4.60</td>\n",
              "      <td>09/04/2018 00:00</td>\n",
              "      <td>38.898318</td>\n",
              "      <td>-77.032800</td>\n",
              "      <td>38.899817</td>\n",
              "      <td>-77.026514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7.30</td>\n",
              "      <td>09/04/2018 00:00</td>\n",
              "      <td>38.888945</td>\n",
              "      <td>-77.039490</td>\n",
              "      <td>38.899817</td>\n",
              "      <td>-77.026514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5.14</td>\n",
              "      <td>09/04/2018 00:00</td>\n",
              "      <td>38.909652</td>\n",
              "      <td>-77.033254</td>\n",
              "      <td>38.918610</td>\n",
              "      <td>-77.035028</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.67</td>\n",
              "      <td>09/04/2018 00:00</td>\n",
              "      <td>38.896667</td>\n",
              "      <td>-76.982929</td>\n",
              "      <td>38.854779</td>\n",
              "      <td>-76.974019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10.27</td>\n",
              "      <td>09/04/2018 00:00</td>\n",
              "      <td>38.897204</td>\n",
              "      <td>-77.008388</td>\n",
              "      <td>38.907244</td>\n",
              "      <td>-77.045287</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   fareamount  ... destination_block_longitude\n",
              "0        4.60  ...                  -77.026514\n",
              "1        7.30  ...                  -77.026514\n",
              "2        5.14  ...                  -77.035028\n",
              "3       11.67  ...                  -76.974019\n",
              "4       10.27  ...                  -77.045287\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uY4lLvmL9kne",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "51577e36-708c-498c-afbb-8393da372568"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3289206 entries, 0 to 3289205\n",
            "Data columns (total 6 columns):\n",
            " #   Column                       Dtype  \n",
            "---  ------                       -----  \n",
            " 0   fareamount                   float64\n",
            " 1   origindatetime_tr            object \n",
            " 2   origin_block_latitude        float64\n",
            " 3   origin_block_longitude       float64\n",
            " 4   destination_block_latitude   float64\n",
            " 5   destination_block_longitude  float64\n",
            "dtypes: float64(5), object(1)\n",
            "memory usage: 150.6+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-kVFhml9p0x",
        "colab_type": "text"
      },
      "source": [
        "## Drop the `origindatetime_tr` column from the data frame. \n",
        "\n",
        "For now you are going to predict the taxi fare just based on the lat/lon coordinates of the pickup and the drop off locations. Remove the `origindatetime_tr` column from the data frame in your working dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhZpJTVZaas_",
        "colab_type": "code",
        "outputId": "5c390ad5-6414-4a9e-dd9f-020287bffa65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "working_df = df.drop('origindatetime_tr', axis = 1)\n",
        "working_df.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3289206, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aA0NkUA_x1M",
        "colab_type": "text"
      },
      "source": [
        "## Sample 10% of your working dataset into a test dataset data frame\n",
        "\n",
        "* **hint:** use the Pandas `sample` function with the dataframe. Specify a value for the `random_state` to achieve reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsh_vPXiZr9J",
        "colab_type": "code",
        "outputId": "25b1cf97-f6c8-45b7-d68b-9a9560b60c71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_df = working_df.sample(frac = 0.10, random_state = 42)\n",
        "test_df.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(328921, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5FschugACN-",
        "colab_type": "text"
      },
      "source": [
        "## Drop the rows that exist in your test dataset from the working dataset to produce a training dataset.\n",
        "\n",
        "* **hint** DataFrame's `drop` function can use index values from a data frame to drop specific rows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CT-b2IlIZ9FP",
        "colab_type": "code",
        "outputId": "9ca1f93d-65f2-43ac-9da0-e754e312c32f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_df = working_df.drop(index = test_df.index)\n",
        "train_df.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2960285, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5R0P1sBeAX15",
        "colab_type": "text"
      },
      "source": [
        "## Define 2 Python lists: 1st for the feature column names; 2nd for the target column name"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s62k_A-Ga-0x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FEATURES = ['origin_block_latitude','origin_block_longitude','destination_block_latitude','destination_block_longitude']\n",
        "TARGET = ['fareamount']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttQDA-m8AgQx",
        "colab_type": "text"
      },
      "source": [
        "## Create `X` and `y` tensors with the values of your feature and target columns in the training dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hX2dlZgpbA6I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = pt.tensor(train_df[FEATURES].values)\n",
        "y = pt.tensor(train_df[TARGET].values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQm_SJFDAqqn",
        "colab_type": "text"
      },
      "source": [
        "## Create a `TensorDataset` instance with the `y` and `X` tensors (in that order)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffqTuheNbLpj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ds = TensorDataset(y, X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElrnaKEtAyEg",
        "colab_type": "text"
      },
      "source": [
        "## Create a `DataLoader` instance specifying a custom batch size\n",
        "\n",
        "A batch size of `2 ** 18 = 262,144` should work well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1YyY4tgbalF",
        "colab_type": "code",
        "outputId": "65d3bde1-d379-4e1f-805e-f36345dbecf6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "BATCH_SIZE = 2 ** 18\n",
        "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE)\n",
        "len(train_dl)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IA-3bXKABCW_",
        "colab_type": "text"
      },
      "source": [
        "## Create a model using `nn.Linear`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vYGtKDeajQk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w = nn.Linear(len(FEATURES), 1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9UvsR9gBGXj",
        "colab_type": "text"
      },
      "source": [
        "## Create an instance of the `AdamW` optimizer for the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPFF7EtFBKes",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = pt.optim.AdamW(w.parameters())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tz7LW-TnBNJu",
        "colab_type": "text"
      },
      "source": [
        "## Declare your `forward`, `loss` and `metric` functions\n",
        "\n",
        "* **hint:** if you are tried of computing MSE by hand you can use `nn.functional.mse_loss` instead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0T3aWJEZdiVH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward(X):\n",
        "  return w(X)\n",
        "\n",
        "def loss(y_pred, y):\n",
        "  mse = nn.functional.mse_loss(y_pred, y)\n",
        "  return mse, mse.sqrt()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfu2ejeoBfpQ",
        "colab_type": "text"
      },
      "source": [
        "## Iterate over the batches returned by your `DataLoader` instance\n",
        "\n",
        "For every step of gradient descent, print out the MSE, RMSE, and the batch index\n",
        "* **hint:** you can use Python's `enumerable` for an iterable\n",
        "* **hint:** the batch returned by the `enumerable` has the same contents as your `TensorDataset` instance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVjF8VYwbATZ",
        "colab_type": "code",
        "outputId": "541119b4-615a-4a1e-ae39-e3647f4a9817",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "for batch_idx, batch in enumerate(train_dl):\n",
        "  y, X = batch\n",
        "  y_pred = forward(X)\n",
        "  mse, rmse = loss(y_pred, y)\n",
        "  mse.backward()\n",
        "  print(\"Loss: \", mse.item(), \" RMSE: \", rmse.item(), \" Batch Idx: \", batch_idx)\n",
        "  optimizer.step()\n",
        "  optimizer.zero_grad()\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss:  1209.8594636144935  RMSE:  34.78303413468258  Batch Idx:  0\n",
            "Loss:  1196.3749611549224  RMSE:  34.58865364761864  Batch Idx:  1\n",
            "Loss:  1178.9680632450134  RMSE:  34.33610436908959  Batch Idx:  2\n",
            "Loss:  1168.8755737473123  RMSE:  34.18882235098648  Batch Idx:  3\n",
            "Loss:  1173.7366406115639  RMSE:  34.25984005525367  Batch Idx:  4\n",
            "Loss:  1164.8815089894983  RMSE:  34.13036051654741  Batch Idx:  5\n",
            "Loss:  1153.9715867701436  RMSE:  33.97015729681191  Batch Idx:  6\n",
            "Loss:  1185.5553946710122  RMSE:  34.43189502003937  Batch Idx:  7\n",
            "Loss:  1135.4230635040444  RMSE:  33.69603928511546  Batch Idx:  8\n",
            "Loss:  1095.7779420329684  RMSE:  33.1025367915054  Batch Idx:  9\n",
            "Loss:  1138.6996221462741  RMSE:  33.74462360356497  Batch Idx:  10\n",
            "Loss:  1140.6698334085045  RMSE:  33.77380395230162  Batch Idx:  11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVl44Jq5CApl",
        "colab_type": "text"
      },
      "source": [
        "## Implement 10 epochs of gradient descent training\n",
        "\n",
        "For every step of gradient descent, printout the MSE, RMSE, epoch index, and batch index.\n",
        "\n",
        "* **hint:** you can call `enumerate(DataLoader)` repeatedly in a `for` loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHtI3TB8ewaF",
        "colab_type": "code",
        "outputId": "4a6f31a7-a55d-4642-ff30-1e7c2b53a0f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for epoch in range(10):\n",
        "  for batch_idx, batch in enumerate(train_dl):\n",
        "    y, X = batch\n",
        "    y_pred = forward(X)\n",
        "    mse, rmse = loss(y_pred, y)\n",
        "    mse.backward()\n",
        "    print(\" Loss: \", mse.item(), \" RMSE: \", rmse.item(), \" Epoch: \", epoch, \" Batch Idx: \", batch_idx)\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Loss:  1024.9110619839025  RMSE:  32.014232178578055  Epoch:  0  Batch Idx:  0\n",
            " Loss:  1012.6604717743405  RMSE:  31.822326624154  Epoch:  0  Batch Idx:  1\n",
            " Loss:  996.8449188331958  RMSE:  31.572850977274697  Epoch:  0  Batch Idx:  2\n",
            " Loss:  987.6885137099952  RMSE:  31.42751205090844  Epoch:  0  Batch Idx:  3\n",
            " Loss:  992.5071579049934  RMSE:  31.504081607071065  Epoch:  0  Batch Idx:  4\n",
            " Loss:  984.6750405196823  RMSE:  31.37953219089925  Epoch:  0  Batch Idx:  5\n",
            " Loss:  974.9285092346067  RMSE:  31.223845202578858  Epoch:  0  Batch Idx:  6\n",
            " Loss:  1004.5903348602639  RMSE:  31.69527306808168  Epoch:  0  Batch Idx:  7\n",
            " Loss:  958.650496555152  RMSE:  30.962081592734556  Epoch:  0  Batch Idx:  8\n",
            " Loss:  922.4228107403496  RMSE:  30.371414368454257  Epoch:  0  Batch Idx:  9\n",
            " Loss:  962.3974534173578  RMSE:  31.022531383131163  Epoch:  0  Batch Idx:  10\n",
            " Loss:  964.6529105287545  RMSE:  31.058862028875986  Epoch:  0  Batch Idx:  11\n",
            " Loss:  858.4349159380986  RMSE:  29.299059983864645  Epoch:  1  Batch Idx:  0\n",
            " Loss:  847.6328338151435  RMSE:  29.114134605293415  Epoch:  1  Batch Idx:  1\n",
            " Loss:  833.5876531210063  RMSE:  28.871918071389132  Epoch:  1  Batch Idx:  2\n",
            " Loss:  825.5358861119048  RMSE:  28.732140298138333  Epoch:  1  Batch Idx:  3\n",
            " Loss:  830.484567383403  RMSE:  28.8181291444015  Epoch:  1  Batch Idx:  4\n",
            " Loss:  823.8083513217384  RMSE:  28.70206179565744  Epoch:  1  Batch Idx:  5\n",
            " Loss:  815.341293486236  RMSE:  28.554181716278197  Epoch:  1  Batch Idx:  6\n",
            " Loss:  843.2696971516214  RMSE:  29.039106342165926  Epoch:  1  Batch Idx:  7\n",
            " Loss:  801.5356524724662  RMSE:  28.311404989376033  Epoch:  1  Batch Idx:  8\n",
            " Loss:  768.746552612432  RMSE:  27.726279097860065  Epoch:  1  Batch Idx:  9\n",
            " Loss:  805.9758617662271  RMSE:  28.38971401346317  Epoch:  1  Batch Idx:  10\n",
            " Loss:  808.6178719149572  RMSE:  28.436207059222177  Epoch:  1  Batch Idx:  11\n",
            " Loss:  711.7334163494663  RMSE:  26.67833233823783  Epoch:  2  Batch Idx:  0\n",
            " Loss:  702.4208286570799  RMSE:  26.503222986215846  Epoch:  2  Batch Idx:  1\n",
            " Loss:  690.165730959116  RMSE:  26.271005518615308  Epoch:  2  Batch Idx:  2\n",
            " Loss:  683.2513654941589  RMSE:  26.139077365013456  Epoch:  2  Batch Idx:  3\n",
            " Loss:  688.3900171259379  RMSE:  26.23718767562442  Epoch:  2  Batch Idx:  4\n",
            " Loss:  682.8842398050439  RMSE:  26.13205387651426  Epoch:  2  Batch Idx:  5\n",
            " Loss:  675.6981975126143  RMSE:  25.994195458075144  Epoch:  2  Batch Idx:  6\n",
            " Loss:  702.0062568370549  RMSE:  26.49540067326884  Epoch:  2  Batch Idx:  7\n",
            " Loss:  664.3488453622102  RMSE:  25.774965477420338  Epoch:  2  Batch Idx:  8\n",
            " Loss:  634.8893711804578  RMSE:  25.197011155699755  Epoch:  2  Batch Idx:  9\n",
            " Loss:  669.5142331263187  RMSE:  25.874973103876236  Epoch:  2  Batch Idx:  10\n",
            " Loss:  672.5505114596515  RMSE:  25.933578840176523  Epoch:  2  Batch Idx:  11\n",
            " Loss:  584.6120244973921  RMSE:  24.1787515082436  Epoch:  3  Batch Idx:  0\n",
            " Loss:  576.7321113522813  RMSE:  24.01524747639052  Epoch:  3  Batch Idx:  1\n",
            " Loss:  566.1898846197923  RMSE:  23.79474489503496  Epoch:  3  Batch Idx:  2\n",
            " Loss:  560.3587580953433  RMSE:  23.671898067019114  Epoch:  3  Batch Idx:  3\n",
            " Loss:  565.6723090737078  RMSE:  23.783866571138255  Epoch:  3  Batch Idx:  4\n",
            " Loss:  561.2719878239441  RMSE:  23.691179536357915  Epoch:  3  Batch Idx:  5\n",
            " Loss:  555.2920471487628  RMSE:  23.564635519115562  Epoch:  3  Batch Idx:  6\n",
            " Loss:  580.0416082876708  RMSE:  24.084052987146304  Epoch:  3  Batch Idx:  7\n",
            " Loss:  546.2418140312959  RMSE:  23.37181666091226  Epoch:  3  Batch Idx:  8\n",
            " Loss:  519.9232354950302  RMSE:  22.80182526674192  Epoch:  3  Batch Idx:  9\n",
            " Loss:  552.0478971919626  RMSE:  23.495699546767334  Epoch:  3  Batch Idx:  10\n",
            " Loss:  555.4316914003675  RMSE:  23.567598337555896  Epoch:  3  Batch Idx:  11\n",
            " Loss:  475.94875001749745  RMSE:  21.816249678106853  Epoch:  4  Batch Idx:  0\n",
            " Loss:  469.3916109460362  RMSE:  21.665447397781477  Epoch:  4  Batch Idx:  1\n",
            " Loss:  460.4337495492804  RMSE:  21.457720045458707  Epoch:  4  Batch Idx:  2\n",
            " Loss:  455.5870736662875  RMSE:  21.344485790627225  Epoch:  4  Batch Idx:  3\n",
            " Loss:  461.0232511512636  RMSE:  21.4714520037948  Epoch:  4  Batch Idx:  4\n",
            " Loss:  457.62418119299684  RMSE:  21.392152327267045  Epoch:  4  Batch Idx:  5\n",
            " Loss:  452.7384162334157  RMSE:  21.27765062767541  Epoch:  4  Batch Idx:  6\n",
            " Loss:  475.9687974002088  RMSE:  21.816709133144  Epoch:  4  Batch Idx:  7\n",
            " Loss:  445.7639118301415  RMSE:  21.113121792623218  Epoch:  4  Batch Idx:  8\n",
            " Loss:  422.3598053250705  RMSE:  20.551394242850545  Epoch:  4  Batch Idx:  9\n",
            " Loss:  452.07503377140324  RMSE:  21.262056198105658  Epoch:  4  Batch Idx:  10\n",
            " Loss:  455.73637696159994  RMSE:  21.347982971737633  Epoch:  4  Batch Idx:  11\n",
            " Loss:  384.16888342871187  RMSE:  19.600226616769305  Epoch:  5  Batch Idx:  0\n",
            " Loss:  378.80256892493674  RMSE:  19.462850996833346  Epoch:  5  Batch Idx:  1\n",
            " Loss:  371.27965572095394  RMSE:  19.26861841754499  Epoch:  5  Batch Idx:  2\n",
            " Loss:  367.30147544255334  RMSE:  19.16511089043195  Epoch:  5  Batch Idx:  3\n",
            " Loss:  372.79489863885755  RMSE:  19.30789731272822  Epoch:  5  Batch Idx:  4\n",
            " Loss:  370.27859678702316  RMSE:  19.242624477628386  Epoch:  5  Batch Idx:  5\n",
            " Loss:  366.36194075007984  RMSE:  19.140583605263448  Epoch:  5  Batch Idx:  6\n",
            " Loss:  388.10719091165726  RMSE:  19.700436312723056  Epoch:  5  Batch Idx:  7\n",
            " Loss:  361.2174651545249  RMSE:  19.00572190564002  Epoch:  5  Batch Idx:  8\n",
            " Loss:  340.48753286678766  RMSE:  18.452304269840873  Epoch:  5  Batch Idx:  9\n",
            " Loss:  367.88348868184784  RMSE:  19.18028906669156  Epoch:  5  Batch Idx:  10\n",
            " Loss:  371.746258674613  RMSE:  19.280722462465274  Epoch:  5  Batch Idx:  11\n",
            " Loss:  307.53323104262665  RMSE:  17.536625417754312  Epoch:  6  Batch Idx:  0\n",
            " Loss:  303.22003010227866  RMSE:  17.413214238108903  Epoch:  6  Batch Idx:  1\n",
            " Loss:  296.97743415258094  RMSE:  17.233033225540446  Epoch:  6  Batch Idx:  2\n",
            " Loss:  293.74855376833614  RMSE:  17.13909431003681  Epoch:  6  Batch Idx:  3\n",
            " Loss:  299.2327001289872  RMSE:  17.298343855091655  Epoch:  6  Batch Idx:  4\n",
            " Loss:  297.47876557034397  RMSE:  17.247572744312283  Epoch:  6  Batch Idx:  5\n",
            " Loss:  294.40474100724157  RMSE:  17.15822662769208  Epoch:  6  Batch Idx:  6\n",
            " Loss:  314.70168214601466  RMSE:  17.739833205135124  Epoch:  6  Batch Idx:  7\n",
            " Loss:  290.84361026090335  RMSE:  17.05413762876632  Epoch:  6  Batch Idx:  8\n",
            " Loss:  272.5454322621107  RMSE:  16.50895006540727  Epoch:  6  Batch Idx:  9\n",
            " Loss:  297.71725709503033  RMSE:  17.254485129815677  Epoch:  6  Batch Idx:  10\n",
            " Loss:  301.7071719596052  RMSE:  17.369719973551824  Epoch:  6  Batch Idx:  11\n",
            " Loss:  244.2816262389225  RMSE:  15.629511388361522  Epoch:  7  Batch Idx:  0\n",
            " Loss:  240.8858171933913  RMSE:  15.520496679983902  Epoch:  7  Batch Idx:  1\n",
            " Loss:  235.77118682609822  RMSE:  15.35484245526792  Epoch:  7  Batch Idx:  2\n",
            " Loss:  233.17570799065714  RMSE:  15.270091944407445  Epoch:  7  Batch Idx:  3\n",
            " Loss:  238.5883662868934  RMSE:  15.446305910698953  Epoch:  7  Batch Idx:  4\n",
            " Loss:  237.48029020655366  RMSE:  15.410395524014096  Epoch:  7  Batch Idx:  5\n",
            " Loss:  235.12655566107537  RMSE:  15.333836951692012  Epoch:  7  Batch Idx:  6\n",
            " Loss:  254.0180846882971  RMSE:  15.937944807543321  Epoch:  7  Batch Idx:  7\n",
            " Loss:  232.91123914183385  RMSE:  15.2614297869444  Epoch:  7  Batch Idx:  8\n",
            " Loss:  216.80629639182558  RMSE:  14.724343665910055  Epoch:  7  Batch Idx:  9\n",
            " Loss:  239.85610814034666  RMSE:  15.487288598729819  Epoch:  7  Batch Idx:  10\n",
            " Loss:  243.90445604032698  RMSE:  15.61744076474526  Epoch:  7  Batch Idx:  11\n",
            " Loss:  192.70215723360258  RMSE:  13.881720254838829  Epoch:  8  Batch Idx:  0\n",
            " Loss:  190.09391463951096  RMSE:  13.787454973254164  Epoch:  8  Batch Idx:  1\n",
            " Loss:  185.9610411326902  RMSE:  13.636753320812481  Epoch:  8  Batch Idx:  2\n",
            " Loss:  183.88966064697962  RMSE:  13.56059219381586  Epoch:  8  Batch Idx:  3\n",
            " Loss:  189.17553535151904  RMSE:  13.754109762231762  Epoch:  8  Batch Idx:  4\n",
            " Loss:  188.60369887582527  RMSE:  13.733306188817945  Epoch:  8  Batch Idx:  5\n",
            " Loss:  186.85496695095244  RMSE:  13.669490369101272  Epoch:  8  Batch Idx:  6\n",
            " Loss:  204.39128185491785  RMSE:  14.296547899927376  Epoch:  8  Batch Idx:  7\n",
            " Loss:  185.76251013227244  RMSE:  13.629472114952671  Epoch:  8  Batch Idx:  8\n",
            " Loss:  171.61986523847253  RMSE:  13.100376530408298  Epoch:  8  Batch Idx:  9\n",
            " Loss:  192.65716118935626  RMSE:  13.880099466118976  Epoch:  8  Batch Idx:  10\n",
            " Loss:  196.70273570923115  RMSE:  14.02507524789907  Epoch:  8  Batch Idx:  11\n",
            " Loss:  151.1685956522507  RMSE:  12.295063873451438  Epoch:  9  Batch Idx:  0\n",
            " Loss:  149.22630889460507  RMSE:  12.21582207199356  Epoch:  9  Batch Idx:  1\n",
            " Loss:  145.93749998961857  RMSE:  12.080459427919891  Epoch:  9  Batch Idx:  2\n",
            " Loss:  144.2894630113812  RMSE:  12.012054903778171  Epoch:  9  Batch Idx:  3\n",
            " Loss:  149.40157298041538  RMSE:  12.222993617785104  Epoch:  9  Batch Idx:  4\n",
            " Loss:  149.26507245443315  RMSE:  12.217408581791522  Epoch:  9  Batch Idx:  5\n",
            " Loss:  148.01492695412446  RMSE:  12.16613853916371  Epoch:  9  Batch Idx:  6\n",
            " Loss:  164.25381447694178  RMSE:  12.816154434031363  Epoch:  9  Batch Idx:  7\n",
            " Loss:  147.84036915121902  RMSE:  12.158962503076445  Epoch:  9  Batch Idx:  8\n",
            " Loss:  135.43937628721352  RMSE:  11.637842424058402  Epoch:  9  Batch Idx:  9\n",
            " Loss:  154.58065955358467  RMSE:  12.433047074373388  Epoch:  9  Batch Idx:  10\n",
            " Loss:  158.57088169376715  RMSE:  12.592493069037884  Epoch:  9  Batch Idx:  11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlrLCc3SCmuk",
        "colab_type": "text"
      },
      "source": [
        "Copyright 2020 CounterFactual.AI LLC. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
      ]
    }
  ]
}